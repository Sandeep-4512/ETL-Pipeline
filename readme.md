**ETL Pipeline â€“ SQL Server | MySQL | MongoDB | AWS S3 | DynamoDB | Python**

This project is a complete End-to-End ETL (Extractâ€“Transformâ€“Load) Pipeline built using Python.
It integrates multiple data sources â€” SQL Server, MySQL, and MongoDB â€” performs data cleaning and transformations using Pandas, and loads the prepared datasets back into target systems such as SQL Server, MySQL, and AWS services like S3 and DynamoDB.

The goal of this project is to demonstrate a production-style ETL workflow with modular code, configuration-driven connections, and scalable handling of heterogeneous data.

ğŸš€ Features

ğŸ”¹ Extraction

Extracts data from:

    SQL Server
    
    MySQL
    
    MongoDB

Config-driven database connections.

Reusable extraction functions using .ini file.

ğŸ”¹ Transformation

    Data cleaning & standardization:
    
    Deduplication
    
    Type corrections
    
    Date parsing
    
    Numeric conversions (e.g., order_amount)
    
    Handling missing or inconsistent values
    
    Dataset merging & unification into a single dataframe.

ğŸ”¹ Loading

Loads processed data into:

    SQL Server (with auto table creation)
    
    MySQL
    
    AWS DynamoDB
    
    AWS S3 (raw file storage)

Supports:

Prepared statements

Automatic schema creation

Insert batching

ğŸ”¹ Additional Capabilities

Modular pipeline functions (Extract â†’ Transform â†’ Load)

Error handling with clear messages

Configurable file paths and database settings

Reusable utility functions

ğŸ“ Project Structure

    ETL-Pipeline/
    â”‚â”€â”€ config/
    â”‚   â””â”€â”€ config.ini              # Database & AWS credentials
    â”‚
    â”‚â”€â”€ src/
    â”‚   â”œâ”€â”€ config_loader.py        # Reads config.ini
    â”‚   â”œâ”€â”€ extract.py              # Extract functions (SQL Server, MySQL, MongoDB)
    â”‚   â”œâ”€â”€ transform.py            # Cleaning & transformation logic
    â”‚   â”œâ”€â”€ load.py                 # Load into SQL Server, MySQL, DynamoDB
    â”‚   â”œâ”€â”€ util.py                 # DB utilities, common helpers
    â”‚   â”œâ”€â”€ main.py                 # Orchestrates full ETL pipeline
    â”‚   â”œâ”€â”€ test.py                 # Testing individual modules
    â”‚   â””â”€â”€ sql_unified_data.csv    # (generated) unified cleaned dataset
    â”‚
    â””â”€â”€ README.md

âš™ï¸ Tech Stack

    Layer	          Technology
    Programming	  Python (Pandas, mysql-connector, pyodbc, boto3, pymongo)
    Databases	  SQL Server, MySQL, MongoDB
    Cloud Services 	  AWS S3, AWS DynamoDB
    Configuration	  INI file-based config loader
    Tooling	          VS Code / PyCharm

**ğŸ§  How the Pipeline Works**

1ï¸âƒ£ Extract

Pulls customer & order data from:

MySQL tables

SQL Server tables

MongoDB collections

All extraction functions return Pandas DataFrames.

2ï¸âƒ£ Transform

Transformations include:

Converting dates to datetime

Cleaning numeric fields (order_amount)

Removing special characters and noise

Handling nulls, duplicates

Merging extracted datasets into sql_unified_df

3ï¸âƒ£ Load

The final cleaned DataFrame is loaded into:

SQL Server (create table if not exists + insert)

MySQL (create table if not exists + insert)

DynamoDB (batch_write_item)

S3 (raw + processed files)

â–¶ï¸ Running the Pipeline

Step 1 â€” Add your config

Update config/config.ini with:

    [mysql]
    host=
    user=
    password=
    database=
    
    [sqlserver]
    server=
    user=
    password=
    database=
    
    [mongodb]
    uri=
    database=
    
    [aws]
    access_key=
    secret_key=
    bucket_name=
    region=

Step 2 â€” Run the pipeline

    python src/main.py

ğŸ§ª Testing Individual Components

You can test each stage independently:

    python src/test.py


The test file includes:

Connection checks

Extract tests

Transform tests

Load tests

ğŸ“Œ Future Enhancements

Add logging with Python logging module

Airflow orchestration

Incremental loading (CDC)

Complete unit test coverage

Docker containerization

ğŸ“ License

This project is open-source under the MIT License.